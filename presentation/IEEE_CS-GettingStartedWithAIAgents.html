<!doctype html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <title>Getting Started with Local AI Agents</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <link rel="stylesheet" href="dist/reset.css">
  <link rel="stylesheet" href="dist/reveal.css">
  <link rel="stylesheet" href="dist/theme/dracula.css" id="theme">

  <!-- Theme used for syntax highlighting of code -->
  <link rel="stylesheet" href="plugin/highlight/monokai.css">

  <style>
    /* Smaller code fonts to fit more lines */
    pre code {
      font-size: 0.85em;
    }

    .container {
      display: flex;
      gap: 1rem;
    }

    .col {
      flex: 1;
    }

    .scrollable-slide {
      height: 800px;
      overflow-y: auto !important;
    }

    ::-webkit-scrollbar {
      width: 6px;
    }

    ::-webkit-scrollbar-track {
      -webkit-box-shadow: inset 0 0 6px rgba(0, 0, 0, 0.3);
    }

    ::-webkit-scrollbar-thumb {
      background-color: #333;
    }

    ::-webkit-scrollbar-corner {
      background-color: #333;
    }
  </style>
</head>

<body>

  <div class="reveal">
    <div class="slides">

      <!-- ==================== TITLE ==================== -->
      <section data-auto-animate>
        <h1>Getting Started with Local AI Agents</h1>
        <h3>IEEE Computer Society</h3>
        <p>üßë‚Äçüíª <strong>Speaker:</strong> Josh Langford<br>üìÖ 2025‚Äë10‚Äë23</p>
      </section>

      <section>
        <h2>Agenda</h2>
        <ul>
          <li>Why Local AI Agents?</li>
          <li>Architecture Overview</li>
          <li>Tools & Technologies</li>
          <li>Live Demo: Building a Code Assistant</li>
          <li>Use Cases & Best Practices</li>
          <li>Challenges & Solutions</li>
          <li>Getting Started Resources</li>
        </ul>
      </section>

      <!-- ==================== MOTIVATION ==================== -->
      <section>
        <h2>Why Build a Local Code‚ÄëAssist Agent?</h2>
        <ul>
          <li>üí∏ No per‚Äëtoken SaaS fees ‚Äì run on your own hardware.</li>
          <li>üîê Data never leaves the machine ‚Äì full privacy.</li>
          <li>‚ö°Ô∏è Latency: response in < 200‚ÄØms on a CPU/GPU.</li>
          <li>üõ†Ô∏è Full control over model version, prompts, and UI.</li>
        </ul>
      </section>

      <!-- ==================== ARCHITECTURE OVERVIEW ==================== -->
      <section data-auto-animate data-auto-animate-id="arch">
        <h2>High‚Äëlevel Architecture</h2>
        <div class="container">
          <div class="col">
            <img class="r-frame" style="background: rgba(255, 255, 255, 0.338);" data-src="assets/AgentWorkflow.png"
              alt="Down arrow">
          </div>
          <div class="col">
            <ul>
              <li>üì¶ <strong>UI layer</strong>: VS‚ÄØCode extension or simple CLI.</li>
              <li>üß† <strong>Agent service</strong>: Python (FastAPI) exposing an HTTP endpoint.</li>
            </ul>
          </div>
        </div>
      </section>

      <section data-auto-animate data-auto-animate-id="arch">
        <h2>High‚Äëlevel Architecture</h2>
        <div class="container">
          <div class="col">
            <img class="r-frame" style="background: rgba(255, 255, 255, 0.338);" data-src="assets/AgentWorkflow.png"
              alt="Down arrow">
          </div>
          <div class="col">
            <ul>
              <li>üîç <strong>Retriever</strong>: file‚Äësystem scan + fuzzy search (e.g. <code>llama‚Äëindex</code>).</li>
              <li>ü§ñ <strong>LLM</strong>: Open‚Äësource code model (CodeLlama‚Äë7B, StarCoder, Mistral‚ÄëCode, etc.).</li>
            </ul>
          </div>
        </div>
      </section>

      <!-- ==================== HARDWARE & SOFTWARE REQUIREMENTS ==================== -->
      <section data-auto-animate data-auto-animate-id="reqs">
        <h2>Hardware & Software Prereqs</h2>
        <table>
          <thead>
            <tr>
              <th>Component</th>
              <th>Minimum</th>
              <th>Recommended</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>CPU</td>
              <td>8‚Äëcore modern x86_64</td>
              <td>16‚Äëcore + AVX2</td>
            </tr>
            <tr>
              <td>GPU</td>
              <td>‚Äî (CPU‚Äëonly inference)</td>
              <td>NVIDIA RTX‚ÄØ3070 / 12‚ÄØGB VRAM or better</td>
            </tr>
            <tr>
              <td>RAM</td>
              <td>16‚ÄØGB</td>
              <td>32‚ÄØGB+</td>
            </tr>
          </tbody>
        </table>
      </section>

      <section data-auto-animate data-auto-animate-id="reqs">
        <h2>Hardware & Software Prereqs</h2>
        <table>
          <thead>
            <tr>
              <th>Component</th>
              <th>Minimum</th>
              <th>Recommended</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>OS</td>
              <td>Linux / macOS / WSL2</td>
              <td>Linux (Ubuntu 22.04)</td>
            </tr>
            <tr>
              <td>Python</td>
              <td>3.10</td>
              <td>3.11</td>
            </tr>
            <tr>
              <td>Libraries</td>
              <td>pip, git</td>
              <td>uv, poetry (optional)</td>
            </tr>
          </tbody>
        </table>
      </section>

      <!-- ==================== SETTING UP THE ENVIRONMENT ==================== -->
      <section>
        <h2>Step‚ÄØ1 ‚Äì Clone the Repository and Run the Install Script</h2>
        <pre><code class="language-bash">
# Create a new directory
git clone https://github.com/jlangfor/ieee-ai-agent.git
cd ieee-ai-agent

# Use the install.sh script to set up the environment
./install.sh
      </code></pre>
      </section>

      <!-- ==================== CHOOSING A MODEL ==================== -->
      <section>
        <h2>Step‚ÄØ2 ‚Äì Download the Model</h2>
        <ul>
          <li><strong>CodeLlama‚Äë7B‚ÄëInstruct</strong> ‚Äì balanced performance/size.</li>
          <li><strong>StarCoder‚ÄëBase‚Äë15B</strong> ‚Äì larger, better at multiline completions.</li>
          <li><strong>Mistral‚ÄëCode‚Äë7B‚ÄëInstruct</strong> ‚Äì recent, good for low‚Äëlatency.</li>
        </ul>
        <p>We‚Äôll use <code>codellama/CodeLlama-7b-Instruct-hf</code> from Ollama.</p>

      </section>
      <section>
        <h2>Download Model</h2>
        <pre><code class="language-bash">
# One‚Äëtime model download
ollama pull codellama:7b
      </code></pre>
      </section>

      <!-- ==================== LOADING THE MODEL ==================== -->
      <section>
        <h2>Step‚ÄØ3 ‚Äì Code Assistant - Part 1: :API Client</h2>
        <pre><code class="language-python" data-line-numbers="1-10|10-20|20-30|30-35">
          # local_code_assistant.py
          import requests
          import json
          import sys
          
          class LocalCodeAssistant:
              def __init__(self, model="codellama:7b", base_url="http://localhost:11434"):
                  self.model = model
                  self.base_url = base_url
                  self.api_url = f"{base_url}/api/generate"
              
              def generate(self, prompt, system_prompt=None):
                  """Send a prompt to the local LLM and get a response."""
                  payload = {
                      "model": self.model,
                      "prompt": prompt,
                      "stream": False
                  }
                  
                  if system_prompt:
                      payload["system"] = system_prompt
                  
                  try:
                      response = requests.post(self.api_url, json=payload, timeout=120)
                      response.raise_for_status()
                      return response.json()["response"]
                  except requests.exceptions.RequestException as e:
                      return f"Error connecting to Ollama: {e}"
      </code></pre>
      </section>

      <!-- ==================== AGENT ACTIONS ==================== -->
      <section>
        <h2>Step‚ÄØ4 ‚Äì Code Assistant - Part 2: Agent Actions</h2>
        <pre><code class="language-python">
      def generate_code(self, description, language="python"):
          """Generate code from a natural language description."""
          system = f"You are an expert {language} programmer. Generate clean, efficient code."
          prompt = f"Write {language} code that: {description}\n\nProvide only the code, no explanations."
          return self.generate(prompt, system)
      
      def explain_code(self, code, language="python"):
          """Explain what a piece of code does."""
          system = f"You are a code reviewer explaining {language} code to developers."
          prompt = f"Explain this {language} code:\n\n{code}"
          return self.generate(prompt, system)
      
      def debug_code(self, code, error_message=""):
          """Help debug code with optional error message."""
          system = "You are a debugging expert. Analyze code and suggest fixes."
          prompt = f"Debug this code:\n\n{code}\n\n"
          if error_message:
              prompt += f"Error message: {error_message}\n\n"
          prompt += "Provide the bug explanation and fixed code."
          return self.generate(prompt, system)
      </code></pre>
      </section>

      <!-- ==================== Code Assistant - Part 3: More Features ==================== -->
      <section>
        <h2>Step‚ÄØ5 ‚Äì Code Assistant - Part 3: More Features</h2>
        <pre><code class="language-python">
      def add_documentation(self, code, language="python"):
          """Add documentation/comments to code."""
          system = f"Add clear documentation and comments to {language} code."
          prompt = f"Add comprehensive documentation to this code:\n\n{code}"
          return self.generate(prompt, system)
      
      def refactor_code(self, code, language="python"):
          """Suggest refactoring improvements."""
          system = f"You are a code quality expert specializing in {language}."
          prompt = f"Refactor this code for better readability and performance:\n\n{code}"
          return self.generate(prompt, system)
      
      def code_review(self, code, language="python"):
          """Perform a code review."""
          system = f"You are a senior {language} developer doing a code review."
          prompt = f"Review this code and provide feedback:\n\n{code}"
          return self.generate(prompt, system)
      </code></pre>
      </section>

      <!-- ==================== Code Assistant - Part 4: CLI Interface ==================== -->
      <section>
        <h2>Step‚ÄØ5 ‚Äì Code Assistant - Part 4: CLI Interface</h2>
        <pre><code class="language-python">
def main():
    assistant = LocalCodeAssistant()
    
    print("ü§ñ Local Code Assistant")
    print("=" * 50)
    print("Commands: generate, explain, debug, document, refactor, review, quit")
    print()
    
    while True:
        command = input("\n> Command: ").strip().lower()
        
        if command == "quit":
            break
        elif command == "generate":
            desc = input("  Description: ")
            lang = input("  Language (default: python): ") or "python"
            print("\nüîß Generating code...\n")
            print(assistant.generate_code(desc, lang))
        elif command == "explain":
            print("  Paste code (end with empty line):")
            code = "\n".join(iter(input, ""))
            print("\nüìñ Explanation:\n")
            print(assistant.explain_code(code))
        # Add other commands similarly...
        else:
            print("Unknown command. Try: generate, explain, debug, document, refactor, review, quit")

if __name__ == "__main__":
    main()
      </code></pre>
      </section>

      <section>
        <h2>Live Demo Time! üé¨</h2>
        <div class="demo-box">
          <p><strong>Let's try:</strong></p>
          <ol class="small-text">
            <li>Generate a Python function for binary search</li>
            <li>Ask the agent to explain it</li>
            <li>Add comprehensive documentation</li>
            <li>Request a refactored version</li>
          </ol>
        </div>
        <div class="highlight-box" style="margin-top: 30px;">
          <strong>Note:</strong> First request may take 10-15 seconds as the model loads into memory. Subsequent
          requests are much faster (1-5 seconds).
        </div>
      </section>

      <!-- Use Cases -->
      <section>
        <h2>Practical Use Cases</h2>
        <div class="container small-text">
          <div class="col">
            <h4>Individual Developers</h4>
            <ul>
              <li>Quick code snippets</li>
              <li>Learning new languages</li>
              <li>Debugging help</li>
              <li>Code documentation</li>
            </ul>
            <h4 style="margin-top: 20px;">Teams</h4>
            <ul>
              <li>Code review assistance</li>
              <li>Consistent coding standards</li>
            </ul>
          </div>
          <div class="col">
            <h4>Enterprise</h4>
            <ul>
              <li>Proprietary code stays internal</li>
              <li>Compliance requirements</li>
              <li>Custom model fine-tuning</li>
              <li>Air-gapped environments</li>
            </ul>
          </div>
        </div>
      </section>

      <!-- Challenges -->
      <section>
        <h2>Challenges & Solutions</h2>
        <div class="pros-cons">
          <div class="cons">
            <h4>Challenges</h4>
            <ul>
              <li><strong>Quality:</strong> Local models < cloud models</li>
              <li><strong>Speed:</strong> Slower without GPU</li>
              <li><strong>Memory:</strong> 7B needs ~8GB RAM</li>
              <li><strong>Context:</strong> Limited context window</li>
            </ul>
          </div>
          <div class="pros">
            <h4>Solutions</h4>
            <ul>
              <li>Use quantized models (Q4, Q5)</li>
              <li>GPU acceleration with CUDA/Metal</li>
              <li>Start with smaller models</li>
              <li>Break tasks into smaller chunks</li>
            </ul>
          </div>
        </div>
      </section>

      <!-- Best Practices -->
      <section>
        <h2>Best Practices</h2>
        <ul>
          <li><strong>Model Selection:</strong> Start with 7B models, upgrade if needed</li>
          <li><strong>Prompt Engineering:</strong> Clear, specific prompts work best</li>
          <li><strong>System Prompts:</strong> Set context and role for better results</li>
          <li><strong>Iterative Refinement:</strong> Ask follow-up questions</li>
          <li><strong>Validation:</strong> Always review generated code</li>
          <li><strong>Version Control:</strong> Track what works, fine-tune prompts</li>
        </ul>
      </section>

      <!-- Advanced Topics -->
      <section>
        <h2>Advanced: RAG for Your Codebase</h2>
        <div class="highlight-box small-text">
          <p><strong>Retrieval-Augmented Generation (RAG)</strong> lets your agent understand your entire codebase:</p>
        </div>
        <img class="r-frame" style="background: rgba(255, 255, 255, 0.338);"
          data-src="assets/rag-architecture-model.jpg" alt="RAG Workflow">
      </section>

      <!-- IDE Integration -->
      <section>
        <h2>IDE Integration Options</h2>
        <ul>
          <li><strong>Continue.dev:</strong> VSCode/JetBrains plugin for local LLMs</li>
          <li><strong>Ollama + Custom Extension:</strong> Build your own</li>
          <li><strong>Language Server Protocol (LSP):</strong> Editor-agnostic</li>
          <li><strong>Vim/Emacs plugins:</strong> Community options available</li>
        </ul>
        <div class="highlight-box" style="margin-top: 30px;">
          <p><strong>Example:</strong> Continue.dev connects to Ollama with zero config and provides inline completions,
            chat, and refactoring in VSCode.</p>
        </div>
      </section>

      <!-- Model Comparison -->
      <section>
        <h2>Model Comparison</h2>
        <table style="font-size: 0.6em; width: 100%;">
          <thead>
            <tr>
              <th>Model</th>
              <th>Size</th>
              <th>Strengths</th>
              <th>RAM Needed</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>CodeLlama 7B</td>
              <td>3.8GB</td>
              <td>Fast, good for snippets</td>
              <td>8GB</td>
            </tr>
            <tr>
              <td>DeepSeek Coder 6.7B</td>
              <td>3.8GB</td>
              <td>Strong coding, fast</td>
              <td>8GB</td>
            </tr>
            <tr>
              <td>CodeLlama 13B</td>
              <td>7.4GB</td>
              <td>Better reasoning</td>
              <td>16GB</td>
            </tr>
            <tr>
              <td>DeepSeek Coder 33B</td>
              <td>19GB</td>
              <td>Production quality</td>
              <td>32GB</td>
            </tr>
            <tr>
              <td>Qwen2.5-Coder 7B</td>
              <td>4.7GB</td>
              <td>Multilingual, recent</td>
              <td>8GB</td>
            </tr>
          </tbody>
        </table>
      </section>

      <!-- Resources -->
      <section>
        <h2>Getting Started Resources</h2>
        <div class="two-column small-text">
          <div>
            <h4>Tools & Platforms</h4>
            <ul>
              <li>Ollama: <a href="https://ollama.com">ollama.com</a></li>
              <li>LM Studio: <a href="https://lmstudio.ai">lmstudio.ai</a></li>
              <li>Continue.dev: <a href="https://continue.dev">continue.dev</a></li>
              <li>HuggingFace: Models & datasets</li>
            </ul>
          </div>
          <div>
            <h4>Learning Resources</h4>
            <ul>
              <li>Ollama Documentation</li>
              <li>HuggingFace Transformers Course</li>
              <li>r/LocalLLaMA community</li>
              <li>GitHub: awesome-local-ai</li>
            </ul>
          </div>
        </div>
        <div class="highlight-box" style="margin-top: 30px;">
          <strong>Demo code available at:</strong> [Your GitHub repo]
        </div>
      </section>

      <!-- Next Steps -->
      <section>
        <h2>Your Next Steps</h2>
        <ol>
          <li>Install Ollama and try <code>ollama run codellama</code></li>
          <li>Experiment with the demo code assistant</li>
          <li>Try different models for your use case</li>
          <li>Integrate with your IDE (Continue.dev)</li>
          <li>Build custom agents for your workflow</li>
          <li>Explore RAG for codebase-aware assistance</li>
        </ol>
        <div class="highlight-box" style="margin-top: 30px;">
          <strong>Remember:</strong> Local AI is rapidly evolving. What's challenging today may be trivial in 6 months!
        </div>
      </section>

      <!-- Q&A -->
      <section>
        <h1>Questions?</h1>
        <p>Thank you for attending!</p>
        <div style="margin-top: 50px;">
          <p>üîó Demo Code: <br /><a
              href="https://github.com/jlangfor/ieee-ai-agent.git">https://github.com/jlangfor/ieee-ai-agent.git</a></p>
          <p>üìß Contact: <br /><a href="mailto:josh.langford@ieee.org">josh.langford@ieee.org</a></p>
          <p>üí¨ Let's discuss local AI!</p>
        </div>
      </section>

    </div>
  </div>

  <script src="dist/reveal.js"></script>
  <script src="plugin/zoom/zoom.js"></script>
  <script src="plugin/notes/notes.js"></script>
  <script src="plugin/search/search.js"></script>
  <script src="plugin/markdown/markdown.js"></script>
  <script src="plugin/highlight/highlight.js"></script>
  <script>

    Reveal.initialize({
      controls: true,
      progress: true,
      center: true,
      hash: true,

      // Learn about plugins: https://revealjs.com/plugins/
      plugins: [RevealZoom, RevealNotes, RevealSearch, RevealMarkdown, RevealHighlight]
    });

  </script>
</body>

</html>