<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <title>Getting Started with Local AI Agents ‚Äì Code‚ÄëAssist Copilot‚Äëlike</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <link rel="stylesheet" href="dist/reset.css">
  <link rel="stylesheet" href="dist/reveal.css">
  <link rel="stylesheet" href="dist/theme/dracula.css" id="theme">

  <!-- Theme used for syntax highlighting of code -->
  <link rel="stylesheet" href="plugin/highlight/monokai.css">

  <style>
    /* Smaller code fonts to fit more lines */
    pre code {font-size: 0.85em;}
    .container {display: flex; gap: 1rem;}
    .col {flex: 1;}
    .scrollable-slide {
        height: 800px;
        overflow-y: auto !important;
    }
    ::-webkit-scrollbar {
        width: 6px;
    }
    ::-webkit-scrollbar-track {
        -webkit-box-shadow: inset 0 0 6px rgba(0,0,0,0.3);
    }
    ::-webkit-scrollbar-thumb {
    background-color: #333;
    }
    ::-webkit-scrollbar-corner {
    background-color: #333;
    }
  </style>
</head>
<body>

<div class="reveal">
  <div class="slides">

    <!-- ==================== TITLE ==================== -->
    <section data-auto-animate>
      <h1>Getting Started with Local AI Agents</h1>
      <h3>Building a Co‚Äëpilot‚Äëstyle code analyser</h3>
      <p>üßë‚Äçüíª  <strong>Speaker:</strong> Josh Langford<br>üìÖ 2025‚Äë10‚Äë23</p>
    </section>

    <!-- ==================== MOTIVATION ==================== -->
    <section>
      <h2>Why Build a Local Code‚ÄëAssist Agent?</h2>
      <ul>
        <li>üí∏ No per‚Äëtoken SaaS fees ‚Äì run on your own hardware.</li>
        <li>üîê Data never leaves the machine ‚Äì full privacy.</li>
        <li>‚ö°Ô∏è Latency: response in < 200‚ÄØms on a CPU/GPU.</li>
        <li>üõ†Ô∏è Full control over model version, prompts, and UI.</li>
      </ul>
    </section>

    <!-- ==================== ARCHITECTURE OVERVIEW ==================== -->
    <section data-auto-animate data-auto-animate-id="arch">
      <h2>High‚Äëlevel Architecture</h2>
      <div class="container">
        <div class="col">
          <img class="r-frame" style="background: rgba(255, 255, 255, 0.338);" data-src="assets/AgentWorkflow.png" alt="Down arrow">
        </div>
        <div class="col">
          <ul>
            <li>üì¶ <strong>UI layer</strong>: VS‚ÄØCode extension or simple CLI.</li>
            <li>üß† <strong>Agent service</strong>: Python (FastAPI) exposing an HTTP endpoint.</li>
          </ul>
        </div>
      </div>
    </section>

    <section data-auto-animate data-auto-animate-id="arch">
      <h2>High‚Äëlevel Architecture</h2>
      <div class="container">
        <div class="col">
          <img class="r-frame" style="background: rgba(255, 255, 255, 0.338);" data-src="assets/AgentWorkflow.png" alt="Down arrow">
        </div>
        <div class="col">
          <ul>
            <li>üîç <strong>Retriever</strong>: file‚Äësystem scan + fuzzy search (e.g. <code>llama‚Äëindex</code>).</li>
            <li>ü§ñ <strong>LLM</strong>: Open‚Äësource code model (CodeLlama‚Äë7B, StarCoder, Mistral‚ÄëCode, etc.).</li>
          </ul>
        </div>
      </div>
    </section>

    <!-- ==================== HARDWARE & SOFTWARE REQUIREMENTS ==================== -->
    <section data-auto-animate data-auto-animate-id="reqs">
      <h2>Hardware & Software Prereqs</h2>
      <table>
        <thead><tr><th>Component</th><th>Minimum</th><th>Recommended</th></tr></thead>
        <tbody>
            <tr><td>CPU</td><td>8‚Äëcore modern x86_64</td><td>16‚Äëcore + AVX2</td></tr>
            <tr><td>GPU</td><td>‚Äî (CPU‚Äëonly inference)</td><td>NVIDIA RTX‚ÄØ3070 / 12‚ÄØGB VRAM or better</td></tr>
            <tr><td>RAM</td><td>16‚ÄØGB</td><td>32‚ÄØGB+</td></tr>
        </tbody>
      </table>
    </section>

    <section data-auto-animate data-auto-animate-id="reqs">
      <h2>Hardware & Software Prereqs</h2>
      <table>
        <thead><tr><th>Component</th><th>Minimum</th><th>Recommended</th></tr></thead>
        <tbody>
            <tr><td>OS</td><td>Linux / macOS / WSL2</td><td>Linux (Ubuntu 22.04)</td></tr>
            <tr><td>Python</td><td>3.10</td><td>3.11</td></tr>
            <tr><td>Libraries</td><td>pip, git</td><td>uv, poetry (optional)</td></tr>
        </tbody>
      </table>
    </section>

    <!-- ==================== SETTING UP THE ENVIRONMENT ==================== -->
    <section>
      <h2>Step‚ÄØ1 ‚Äì Create a Python Virtual Environment</h2>
      <pre><code class="language-bash">
# Create a new directory
git clone https://github.com/jlangfor/ieee-ai-agent.git
cd ieee-ai-agent

# Recommended: use uv (fast) or python -m venv
python -m venv .venv
source .venv/bin/activate

# Upgrade pip & install core deps
pip install -U pip wheel
pip install -r requirements.txt
      </code></pre>
</section>
<section>
      <p><strong>requirements.txt</strong> (kept minimal for the demo):</p>
      <pre><code class="language-text">
fastapi[all]==0.112.0
uvicorn[standard]==0.30.1
torch==2.4.0  # CPU‚Äëonly wheel works on Linux/macOS
transformers==4.44.2
sentencepiece==0.2.0
pydantic==2.8.2
llama-index==0.10.38
   </code></pre>
    </section>

    <!-- ==================== CHOOSING A MODEL ==================== -->
    <section>
      <h2>Step‚ÄØ2 ‚Äì Pick a Code‚ÄëSpecialised LLM</h2>
      <ul>
        <li><strong>CodeLlama‚Äë7B‚ÄëInstruct</strong> ‚Äì balanced performance/size.</li>
        <li><strong>StarCoder‚ÄëBase‚Äë15B</strong> ‚Äì larger, better at multiline completions.</li>
        <li><strong>Mistral‚ÄëCode‚Äë7B‚ÄëInstruct</strong> ‚Äì recent, good for low‚Äëlatency.</li>
      </ul>
      <p>We‚Äôll use <code>codellama/CodeLlama-7b-Instruct-hf</code> from HuggingFace.</p>

    </section>
    <section>
        <h2>Download Model</h2>
      <pre><code class="language-bash">
# One‚Äëtime model download (cached in ~/.cache/huggingface)
pip install huggingface_hub==0.24.5
git lfs install

# Use HF CLI to download (optional)
huggingface-cli download codellama/CodeLlama-7b-Instruct-hf \
    --local-dir ./models/codellama-7b
      </code></pre>
    </section>

    <!-- ==================== LOADING THE MODEL ==================== -->
    <section>
      <h2>Step‚ÄØ3 ‚Äì Load the Model Efficiently</h2>
      <p>We‚Äôll use <code>transformers</code> with <code>torch.compile</code> and <code>bnb</code> 4‚Äëbit quantisation for a 7‚ÄØB model on a 12‚ÄØGB GPU.</p>
      <pre><code class="language-python" data-line-numbers="1-10|10-20|20-30|30-40">
# file: src/model.py
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

class CodeLLM:
    def __init__(self, repo_id: str = "codellama/CodeLlama-7b-Instruct-hf"):
        # 4‚Äëbit quantisation ‚Äì massive memory saving
        quant_cfg = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_use_double_quant=True,
        )
        self.tokenizer = AutoTokenizer.from_pretrained(repo_id)
        self.model = AutoModelForCausalLM.from_pretrained(
            repo_id,
            device_map="auto",
            quantization_config=quant_cfg,
            torch_dtype=torch.float16,
            trust_remote_code=True,
        )
        # Compile for faster inference on recent PyTorch
        if torch.cuda.is_available():
            self.model = torch.compile(self.model, mode="max-autotune")

    def complete(self, prompt: str, max_new_tokens: int = 256,
                 temperature: float = 0.2, stop: list[str] | None = None) -> str:
        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.model.device)
        generated = self.model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            temperature=temperature,
            do_sample=False,
            pad_token_id=self.tokenizer.eos_token_id,
            stop_token_ids=[self.tokenizer.convert_tokens_to_ids(s) for s in (stop or [])],
        )
        text = self.tokenizer.decode(generated[0], skip_special_tokens=True)
        # Strip the original prompt
        return text[len(prompt):].lstrip()
      </code></pre>
    </section>

    <!-- ==================== CONTEXT RETRIEVAL ==================== -->
    <section>
      <h2>Step‚ÄØ4 ‚Äì Retrieve Relevant Project Context</h2>
      <p>Instead of sending the whole repository, we embed nearby files and perform a similarity search (RAG).</p>
      <pre><code class="language-python">
# file: src/retriever.py
from pathlib import Path
from typing import List
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, StorageContext, load_index_from_storage
from llama_index.embeddings.huggingface import HuggingFaceEmbedding

class CodeRetriever:
    def __init__(self, project_root: Path, embed_model: str = "sentence-transformers/all-MiniLM-L6-v2"):
        self.project_root = project_root
        self.embedding_model = HuggingFaceEmbedding(model_name=embed_model)
        self.index_path = project_root / ".local_index"

        if self.index_path.exists():
            storage_ctx = StorageContext.from_defaults(persist_dir=self.index_path)
            self.index = load_index_from_storage(storage_ctx)
        else:
            documents = SimpleDirectoryReader(input_files=self._collect_source_files()).load_data()
            self.index = VectorStoreIndex.from_documents(
                documents, embed_model=self.embedding_model
            )
            self.index.storage_context.persist(persist_dir=self.index_path)

    def _collect_source_files(self) -> List[Path]:
        # Include only typical source extensions, ignore venv/.git etc.
        exts = {".py", ".js", ".ts", ".cpp", ".c", ".java", ".go", ".rs", ".tsx", ".jsx"}
        ignore_dirs = {".git", "__pycache__", "node_modules", ".venv", ".local_index"}
        files = []
        for path in self.project_root.rglob("*"):
            if path.is_dir() and path.name in ignore_dirs:
                continue
            if path.suffix in exts:
                files.append(path)
        return files

    def retrieve(self, query: str, top_k: int = 5) -> str:
        """Return concatenated relevant chunks."""
        retriever = self.index.as_retriever(similarity_top_k=top_k)
        results = retriever.retrieve(query)
        # Simple concatenation with source file markers
        context = ""
        for node in results:
            context += f"--- {node.metadata.get('source', 'unknown')} ---\\n"
            context += node.text + "\\n\\n"
        return context
      </code></pre>
    </section>

    <!-- ==================== AGENT SERVICE (FASTAPI) ==================== -->
    <section>
      <h2>Step‚ÄØ5 ‚Äì Expose the Agent via HTTP (FastAPI)</h2>
      <pre><code class="language-python">
# file: src/api.py
import pathlib
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from .model import CodeLLM
from .retriever import CodeRetriever

app = FastAPI(title="Local Code‚ÄëAssist Agent")

# Initialise once at import time
PROJECT_ROOT = pathlib.Path.cwd()
llm = CodeLLM()
retriever = CodeRetriever(project_root=PROJECT_ROOT)

class CompletionRequest(BaseModel):
    # The snippet the user is editing (may be incomplete)
    code: str
    # Cursor offset in characters from the beginning of `code`
    cursor: int
    # Optional file path (used for better retrieval)
    file_path: str | None = None

@app.post("/complete")
def complete(req: CompletionRequest):
    # 1Ô∏è‚É£ Build retrieval query ‚Äì we use the code + file name
    query = f"File: {req.file_path or 'unknown'}\\nSnippet: {req.code[:req.cursor]}"
    context = retriever.retrieve(query, top_k=5)

    # 2Ô∏è‚É£ Construct prompt (few‚Äëshot style)
    prompt = f"""You are an AI coding assistant. Use the provided context to generate the most likely continuation for the user's code.
### Context
{context}
### User code (cursor at ‚¨áÔ∏è)
{req.code[:req.cursor]}‚¨áÔ∏è
### Completion (only raw code, no explanations)"""
    # 3Ô∏è‚É£ Call the model
    try:
        completion = llm.complete(prompt, max_new_tokens=256, temperature=0.1,
                                 stop=["```", "<|endoftext|>"])
        return {"completion": completion}
    except Exception as exc:
        raise HTTPException(status_code=500, detail=str(exc))
      </code></pre>
      <p>Run with:</p>
      <pre><code class="language-bash">
uvicorn src.api:app --host 0.0.0.0 --port 8000 --workers 1
      </code></pre>
    </section>

    <!-- ==================== SIMPLE CLI CLIENT ==================== -->
    <section>
      <h2>Step‚ÄØ6 ‚Äì Quick CLI Demo</h2>
      <pre><code class="language-python">
# file: cli.py
import argparse, json, sys, pathlib, requests

def main():
    parser = argparse.ArgumentParser(description="Local Copilot CLI")
    parser.add_argument("file", type=pathlib.Path, help="File to edit")
    parser.add_argument("-l", "--line", type=int, default=1, help="Line (1‚Äëbased) where cursor is")
    parser.add_argument("-c", "--col", type=int, default=1, help="Column (1‚Äëbased) of cursor")
    args = parser.parse_args()

    src = args.file.read_text(encoding="utf-8")
    lines = src.splitlines()
    # Compute absolute char offset
    cursor = sum(len(l)+1 for l in lines[:args.line-1]) + args.col-1

    payload = {
        "code": src,
        "cursor": cursor,
        "file_path": str(args.file)
    }
    resp = requests.post("http://127.0.0.1:8000/complete", json=payload)
    if resp.ok:
        out = resp.json()["completion"]
        print("\\n--- Completion ---\\n")
        print(out)
    else:
        print("Error:", resp.text, file=sys.stderr)

if __name__ == "__main__":
    main()
      </code></pre>
    </section>

    <!-- ==================== INTEGRATING WITH VS CODE ==================== -->
    <section>
      <h2>Step‚ÄØ7 ‚Äì VS‚ÄØCode Extension (Conceptual)</h2>
      <ul>
        <li>Language Server Protocol (LSP) or plain <code>vscode.extension.api</code>.</li>
        <li>Register a <code>completionItemProvider</code> for the desired languages.</li>
        <li>On <code>provideCompletionItems</code>, send the current document + cursor to the FastAPI endpoint and return the result.</li>
      </ul>

      <pre><code class="language-javascript">
// src/extension.js (simplified)
const vscode = require('vscode');
const fetch = require('node-fetch');

function activate(context) {
  const provider = vscode.languages.registerCompletionItemProvider(
    [{ scheme: 'file', language: 'python' }, { language: 'javascript' }],
    {
      async provideCompletionItems(document, position, token, context) {
        const text = document.getText();
        const offset = document.offsetAt(position);
        const response = await fetch('http://127.0.0.1:8000/complete', {
          method: 'POST',
          headers: { 'Content-Type': 'application/json' },
          body: JSON.stringify({
            code: text,
            cursor: offset,
            file_path: document.fileName,
          }),
        });
        if (!response.ok) return null;
        const data = await response.json();
        return [new vscode.CompletionItem(data.completion, vscode.CompletionItemKind.Snippet)];
      },
    },
    '.' // trigger on any character
  );
  context.subscriptions.push(provider);
}
exports.activate = activate;
      </code></pre>
      <p>Package with <code>vsce</code>, install locally, and you have a Copilot‚Äëlike experience that never leaves your machine.</p>
    </section>

    <!-- ==================== PERFORMANCE TUNING ==================== -->
    <section>
      <h2>Performance & Optimisation Tips</h2>
      <ul>
        <li><strong>Quantisation</strong> ‚Äì 4‚Äëbit (nf4) is best for 7‚ÄØB; 8‚Äëbit if GPU memory < 12‚ÄØGB.</li>
        <li><strong>Compile / TorchScript</strong> ‚Äì <code>torch.compile</code> gives 1.5‚Äë2√ó speedup on recent PyTorch releases.</li>
        <li><strong>Batch retrieval</strong> ‚Äì Retrieve once per file, cache per‚Äësession.</li>
        <li><strong>Async FastAPI</strong> ‚Äì Use <code>async def</code> and <code>httpx.AsyncClient</code> for lower latency under load.</li>
        <li><strong>Prompt caching</strong> ‚Äì Store the last <code>n</code> prompts + outputs in an LRU cache for rapid repeat queries.</li>
      </ul>
    </section>

    <!-- ==================== SECURITY & PRIVACY ==================== -->
    <section>
      <h2>Security & Privacy Considerations</h2>
      <ul>
        <li>Run the service behind <code>localhost</code> or a Unix socket ‚Äì no external exposure.</li>
        <li>Never persist raw user code on disk; only keep embeddings (they are non‚Äëreversible).</li>
        <li>Validate incoming JSON schema (Pydantic) to avoid injection attacks.</li>
        <li>If you expose via Docker, set <code>--network=host</code> only if you trust the host.</li>
      </ul>
    </section>

    <!-- ==================== EXTENSIONS & Next Steps ==================== -->
    <section>
      <h2>Extending the Agent</h2>
      <ul>
        <li><strong>Multi‚Äëturn chat</strong> ‚Äì keep a conversation history in memory for better context.</li>
        <li><strong>Static analysis hooks</strong> ‚Äì run <code>ruff</code> or <code>pylint</code> on the generated code before returning.</li>
        <li><strong>Fine‚Äëtuning</strong> ‚Äì collect your own snippets and use <code>trl</code> to further adapt the model.</li>
        <li><strong>Multi‚Äëmodal</strong> ‚Äì add a code‚Äëgraph (AST) embedding for deeper understanding.</li>
      </ul>
    </section>

    <section>
      <h2>Resources</h2>
      <ul>
        <li>ü§ó HuggingFace model hub ‚Äì <a href="https://huggingface.co/codellama">codellama</a></li>
        <li>LangChain & LlamaIndex ‚Äì RAG libraries.</li>
        <li>FastAPI docs ‚Äì <a href="https://fastapi.tiangolo.com/">fastapi.tiangolo.com</a></li>
        <li>VS Code Extension Authoring ‚Äì <a href="https://code.visualstudio.com/api">code.visualstudio.com/api</a></li>
        <li>Quantisation guide ‚Äì <a href="https://github.com/TimDettmers/bitsandbytes">bitsandbytes</a></li>
      </ul>
    </section>

    <section>
      <h2>Thank you! üéâ</h2>
      <p>Happy hacking with your own local AI coding assistant.</p>
      <p>Questions? <a href="mailto:you@example.com">you@example.com</a></p>
    </section>

  </div>
</div>

<script src="dist/reveal.js"></script>
<script src="plugin/zoom/zoom.js"></script>
<script src="plugin/notes/notes.js"></script>
<script src="plugin/search/search.js"></script>
<script src="plugin/markdown/markdown.js"></script>
<script src="plugin/highlight/highlight.js"></script>
<script>

  Reveal.initialize({
    controls: true,
    progress: true,
    center: true,
    hash: true,

    // Learn about plugins: https://revealjs.com/plugins/
    plugins: [ RevealZoom, RevealNotes, RevealSearch, RevealMarkdown, RevealHighlight ]
  });

</script>
</body>
</html>